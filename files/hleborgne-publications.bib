@string{PAMI = "IEEE T. Pattern Analysis and Machine Intelligence"}
@string{IJCV = "International Journal of Computer Vision"}
@string{CVIU = "International Journal of Computer Vision and Image Understanding"}

@string{CVPR = "Computer Vision and Pattern Recognition (CVPR)"}
@string{ECCV = "European Conference on Computer Vision (ECCV)"}
@string{ICCV = "International Conference on Computer Vision (ICCV)"}

@string{ACCV = "Asian Conference on Computer Vision (ACCV)"}
@string{BMVC = "British Machine Vision Conference (BMVC)"}
@string{ICIP = "International Conference on Image Processing"}
@string{ICPR = "International Conference on Pattern Recognition (ICPR)"}

@string{ICMR = "International Conference on Multimedia Retrieval (ICMR)"}
@string{ACMMM = "International Conference on Multimedia"}

@string{NIPS = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string{ICLR = "International Conference on Learning Representations (ICLR)"}
@string{ICML = "International Conference on Machine Learning (ICML)"}

@string{SIGIR = "SIGIR Conference on Research and Development in Information Retrieval (SIGIR)"}
@string{ECIR = "European Conference on Information Retrieval (ECIR)"}
@string{ECMLKDD= "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"}

@string{arXiv  = "arXiv"}

@unpublished{grimal2023tiam,
      title={TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation}, 
      author={Paul Grimal and Hervé Le Borgne and Olivier Ferret and Julien Tourille},
      year={2023},
      journal={arXiv 2307.05134},
      note={Preprint.}
      url = "https://arxiv.org/abs/2307.05134",
      url_PDF = "https://arxiv.org/pdf/2307.05134.pdf",
      abstract = "The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic."
}

@inproceedings{karaliolios2023gpl,
  author          = {Nikolaos Karaliolios and Florian Chabot and Camille Dupont and Hervé Le Borgne and Romaric Audigier and Quoc-Cuong Pham},
  booktitle       = ICIP,
  title           = {Generalized pseudo-labeling in consistency regularization for semi-supervised learning},
  year            = {2023},
  keywords = {conf-int}
}


@article{doubinsky2022prl,
  author     = {Perla Doubinsky and
               Nicolas Audebert and
               Michel Crucianu and
               Herv{\'{e}} Le Borgne},
  title      = {Multi-Attribute Balanced Sampling for Disentangled {GAN} Controls},
  journal    = {Pattern Recognition Letters},
  volume     = {162},
  pages      = {56-62},
  year       = {2022},
  doi        = {https://doi.org/10.1016/j.patrec.2022.08.012},
  url        = "https://www.sciencedirect.com/science/article/abs/pii/S0167865522002501",
  url_arXiv  = "https://arxiv.org/abs/2111.00909",
  url_PDF    = "https://arxiv.org/pdf/2111.00909.pdf",
  url_Code   = "https://github.com/perladoubinsky/balanced_sampling_gan_controls",
  abstract   = "Various controls over the generated data can be extracted from the latent space of a pre-trained GAN, as it implicitly encodes the semantics of the training data. The discovered controls allow to vary semantic attributes in the generated images but usually lead to entangled edits that affect multiple attributes at the same time. Supervised approaches typically sample and annotate a collection of latent codes, then train classifiers in the latent space to identify the controls. Since the data generated by GANs reflects the biases of the original dataset, so do the resulting semantic controls. We propose to address disentanglement by subsampling the generated data to remove over-represented co-occuring attributes thus balancing the semantics of the dataset before training the classifiers. We demonstrate the effectiveness of this approach by extracting disentangled linear directions for face manipulation on two popular GAN architectures, PGGAN and StyleGAN, and two datasets, CelebAHQ and FFHQ. We show that this approach outperforms state-of-the-art classifier-based methods while avoiding the need for disentanglement-enforcing post-processing.",
  keywords   = {mine}
}

